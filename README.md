马尔可夫过程



一、基础信息 介绍篇
1.1 什么是概率图模型？


概率图模型（Probabilistic Graphical Model， PGM），简称图模型（Graphical Model，GM），是指一种用图结构来描述多元随机变量之间条件独立性的概率模型（注意条件独立性），从而给研究高维空间的概率模型带来了很大的便捷性。



1.2 什么是 随机场？


每个位置按照某种分布随机赋予一个值 所构成 的 整体。



二、马尔可夫过程 介绍篇
2.1 什么是 马尔可夫过程？


假设一个随机过程中， 时刻的状态的条件发布，只与其前一状态 相关，即：







则将其称为 马尔可夫过程。





2.2 马尔可夫过程 的核心思想 是什么？


对于 马尔可夫过程 的 思想，用一句话去概括：当前时刻状态 仅与上一时刻状态相关，与其他时刻不相关。

可以从 马尔可夫过程 图 去理解，由于 每个状态间 是以 有向直线连接，也就是 当前时刻状态 仅与上一时刻状态相关。



三、隐马尔科夫算法 篇
3.1 隐马尔科夫算法 介绍篇
3.1.1 隐马尔科夫算法 是什么？


隐马尔科夫算法是对含有未知参数（隐状态）的马尔可夫链进行建模的生成模型，如下图所示：



在隐马尔科夫模型中，包含隐状态 和 观察状态，隐状态  对于观察者而言是不可见的，而观察状态  对于观察者而言是可见的。隐状态间存在转移概率，隐状态 到对应的观察状态  间存在输出概率。



3.1.2 隐马尔科夫算法 中 两个序列 是什么？






两序列
隐藏序列：隐状态 ii​ 对于观察者而言是不可见的
观测序列：oi​ 对于观察者而言是可见的


3.1.3 隐马尔科夫算法 中 三个矩阵 是什么？


初始状态矩阵：每个标签的概率矩阵
发射状态矩阵：一个字变成每个标签的概率 B=[bij​]N×M​（N为隐藏状态集元素个数，M为观测集元素个数），其中bij​=P(ot​∣it​)，(ot​为第i个观测节点 ，it​ 为第i个隐状态节点,即所谓的观测概率（发射概率）；
状态转移级证：标签到每个标签的概率 A=[aij​]N×N​ （N 表示隐藏状态集元素的个数），其中 aij​=P(it+1​∣it​)，it​ 即第i个隐状态节点，即所谓的状态转移；


3.1.4 隐马尔科夫算法 中 两个假设 是什么？


齐次马尔可夫性假设：即假设隐藏的马尔科夫链在任意时刻 t 的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻 t 无关；






观测独立性假设：即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。




3.1.5 隐马尔科夫算法 中 工作流程 是什么？


隐状态节点  是不能直接观测到的数据节点，  才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系；
在的指导下生成下一个隐状态节点;
在的指导下生成依赖于该的观测节点;
深层次理解：由于 为有向图，而且属于生成式模型，直接对联合概率分布建模




3.2 隐马尔科夫算法 模型计算过程篇
3.2.1 隐马尔科夫算法 序列概率计算过程 是什么样的？


思想
如何对一条序列计算其整体的概率。即目标是计算出 P(O∣λ) ;

给定模型 λ=(A,B,π) 和观测序列 O=(o1,o2,...,oT) ，计算在模型 λ 下观测序列 O 出现的概率 P(O∣λ)

常用方法
直接计算法（穷举搜索）
由于有隐藏的状态序列 I 的存在，我们是无法计算 P(O∣λ) 的。一种常见的做法是把 I 边缘掉，即 P(O∣λ)=∑(P(O,I∣λ)) ，当然，这种计算复杂度非常高，为 O(TN2)

前向算法
减少计算量的原因在于每一次计算直接引用前一个时刻的计算结果，避免重复计算，计算复杂度将为O(T2∗N)

后向算法


3.2.2 隐马尔科夫算法 学习训练过程 是什么样的？




思想
找出数据的分布情况，也就是模型参数的确定；

已知观测序列 O=(o1,o2,...,oT) ，估计模型 λ=(A,B,π) 参数，使得在该模型下观测序列概率 P(O∣λ) 最大，即用极大似然估计的方法估计参数

常用方法
极大似然估计：该算法在训练数据是会 将 观测状态序列 O 和 隐状态序列 I;
Baum-Welch(前向后向)：该算法在训练数据是只会 将 观测状态序列 O;


3.2.3 隐马尔科夫算法 序列标注（解码）过程 是什么样的？


思想
也就是“预测过程”，通常称为解码过程。在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个







常用方法：Viterbi算法
Viterbi计算有向无环图的一条最大路径：





3.3 HMM模型三个基本问题的联系？


三个基本问题 存在 渐进关系。首先，要学会用前向算法和后向算法算观测序列出现的概率，然后用Baum-Welch算法求参数的时候，某些步骤是需要用到前向算法和后向算法的，计算得到参数后，我们就可以用来做预测了。因此可以看到，三个基本问题，它们是渐进的，对于做NLP的同学来说，应用HMM模型做解码任务应该是最终的目的。



3.4 隐马尔科夫算法 问题篇


因为HMM模型其实它简化了很多问题，做了某些很强的假设，如齐次马尔可夫性假设和观测独立性假设，做了假设的好处是，简化求解的难度，坏处是对真实情况的建模能力变弱了。

在序列标注问题中，隐状态（标注）不仅和单个观测状态相关，还和观察序列的长度、上下文等信息相关。例如词性标注问题中，一个词被标注为动词还是名词，不仅与它本身以及它前一个词的标注有关，还依赖于上下文中的其他词。



参考资料


条件随机场CRF
朴素贝叶斯(NB)、逻辑回归(LR)、隐马尔科夫模型(HMM)、条件随机场(CRF)
学习笔记：条件随机场（CRF）
如何轻松愉快地理解条件随机场（CRF）？
概率图模型体系：HMM、MEMM、CRF
CRF 视频介绍
概率图模型（二）：捋一捋HMM模型
概率图模型（三）：理一理CRF模型
